# Book Search Engine - Stage 3

[![My Skills](https://go-skill-icons.vercel.app/api/icons?i=java,maven,idea,docker)](https://go-skill-icons.vercel.app/api/) &nbsp;![Architecture Badge](assets/badges/hazelcastv3.1.svg) &nbsp;![Architecture Badge](assets/badges/activemq.svg) &nbsp;[![My Skills](https://go-skill-icons.vercel.app/api/icons?i=nginx)](https://go-skill-icons.vercel.app/api/) &nbsp;[![My Skills](https://skills.syvixor.com/api/icons?perline=15&i=apachejmeter)](https://go-skill-icons.vercel.app/api/) &nbsp;[![My Skills](https://go-skill-icons.vercel.app/api/icons?i=github)](https://go-skill-icons.vercel.app/api/)

## ğŸ“‘ Tabla de contenidos

- [âœ¨ DescripciÃ³n del proyecto](#-descripciÃ³n-del-proyecto)
- [ğŸ§­ Contexto arquitectÃ³nico](#-contexto-arquitectÃ³nico)
- [ğŸ—ºï¸ Diagrama de arquitectura](#ï¸-diagrama-de-arquitectura)
- [ğŸ”§ Instrucciones de construcciÃ³n y ejecuciÃ³n](#-instrucciones-de-construcciÃ³n-y-ejecuciÃ³n)
- [ğŸ“Š Benchmarking](#-benchmarking-pruebas-de-rendimiento)
- [ğŸ¥ VÃ­deo de demostraciÃ³n](#-vÃ­deo-de-demostraciÃ³n)
- [ğŸ‘¥ Autores](#-autores)



## âœ¨ DescripciÃ³n del proyecto

Este proyecto implementa una **arquitectura de motor de bÃºsqueda distribuida, tolerante a fallos y escalable horizontalmente**. El objetivo es proporcionar una plataforma completa de bÃºsqueda capaz de manejar cargas de trabajo de ingestiÃ³n, indexaciÃ³n y consulta a travÃ©s de mÃºltiples nodos cooperando entre sÃ­.

El sistema estÃ¡ diseÃ±ado como un conjunto de servicios desplegados en mÃºltiples nodos y contenedores. Soporta **ingestiÃ³n paralela de documentos, indexaciÃ³n distribuida y bÃºsquedas de baja latencia** bajo carga creciente, manteniÃ©ndose operativo ante fallos parciales. La escalabilidad y resiliencia se logran mediante replicaciÃ³n, comunicaciÃ³n asÃ­ncrona y comparticiÃ³n de datos en memoria.

CaracterÃ­sticas arquitectÃ³nicas clave:

- **IngestiÃ³n distribuida (crawling)**: mÃºltiples instancias descargan documentos en paralelo y los almacenan en un datalake replicado.
- **IndexaciÃ³n asÃ­ncrona** coordinada mediante un broker de mensajerÃ­a, permitiendo procesar documentos de forma independiente y fiable.
- **Ãndice invertido distribuido en memoria**, implementado con Hazelcast, particionado y replicado en el clÃºster para consultas rÃ¡pidas y tolerancia a fallos.
- **Capa de bÃºsqueda balanceada por carga** usando Nginx, que distribuye peticiones entre instancias y maneja automÃ¡ticamente caÃ­das de nodos.

Todo el sistema es desplegado con **Docker Compose**, permitiendo ejecuciones reproducibles en cualquier entorno. Se incluyen experimentos de benchmarking e inyecciÃ³n de fallos para evaluar escalabilidad, rendimiento y recuperaciÃ³n.

## ğŸ§­ Contexto arquitectÃ³nico

Este proyecto (Stage 3) evoluciona a partir del [**Stage 2**](https://github.com/ayozeruanoalc/stage_2), transformando una soluciÃ³n mononodo en un sistema verdaderamente distribuido y preparado para funcionar en clÃºster:

- **De mononodo a clÃºster distribuido**: Stage 2 era una soluciÃ³n mononodo, sin carÃ¡cter distribuido real; Stage 3 estÃ¡ diseÃ±ada para ejecutarse en mÃºltiples nodos cooperando entre sÃ­, con escalabilidad horizontal y replicaciÃ³n de datos.
- **Tolerancia a fallos avanzada**: mientras que en Stage 2 la caÃ­da del nodo implicaba pÃ©rdida de servicio, Stage 3 combina Hazelcast y ActiveMQ para eliminar puntos Ãºnicos de fallo y permitir que el sistema siga respondiendo incluso ante la caÃ­da de uno o varios nodos.
- **Ingesta y procesamiento asÃ­ncrono mÃ¡s eficiente**: el pipeline de ingestiÃ³n e indexaciÃ³n pasa a estar desacoplado y coordinado mediante mensajerÃ­a, permitiendo que crawlers e indexers trabajen en paralelo de forma fiable.
- **Almacenamiento y consultas distribuidas**: en Stage 2 el Ã­ndice invertido se mantenÃ­a localmente en MongoDB; en Stage 3 se reemplaza por un Ã­ndice invertido en memoria distribuido y replicado mediante Hazelcast, garantizando bÃºsquedas rÃ¡pidas, consistentes y sin punto Ãºnico de fallo.
- **Despliegue reproducible y portable**: todos los servicios se contenerizan y orquestan con Docker Compose, facilitando levantar el mismo clÃºster completo en cualquier entorno de pruebas o laboratorio.

## ğŸ—ºï¸ Diagrama de arquitectura

![Diagrama de arquitectura - Stage 3](docs/architecture-stage3.png)

## ğŸ”§ Instrucciones de ConstrucciÃ³n y EjecuciÃ³n

### ğŸ“Œ Requisitos Previos

AsegÃºrate de que las siguientes herramientas estÃ©n instaladas en todos los nodos que participarÃ¡n en el cluster:

- **Java JDK 17**<br>Verifica con:
  ```bash
  java -version
  ```

- **Apache Maven 3.6+**<br>Verifica con:
  ```bash
  mvn -v
  ```

- **Docker Desktop**

- `curl` (opcional, para comprobaciones rÃ¡pidas de endpoints y estado)

### ğŸ— ConstrucciÃ³n

Todos los servicios se construyen a partir de un proyecto Maven multi-mÃ³dulo. Antes de ejecutar el cluster por primera vez, compila y empaqueta todos los servicios ejecutando el siguiente comando **desde el directorio raÃ­z del repositorio**:
```bash
mvn clean package
```
Este paso genera los archivos JAR ejecutables requeridos por cada microservicio. Las imÃ¡genes de Docker reutilizarÃ¡n estos artefactos durante el inicio de los contenedores.

### âš™ ConfiguraciÃ³n de Servicios (Docker Compose)

El sistema se despliega usando **Docker Compose** y se configura mediante variables de entorno definidas en el archivo `docker-compose.yml`. Cada servicio debe estar correctamente parametrizado para poder unirse al cluster en memoria de Hazelcast, descubrir otros miembros del cluster, conectarse al broker central de ActiveMQ si es necesario, etc.

Todos los marcadores `xxx` deben reemplazarse con la **direcciÃ³n IP de la mÃ¡quina donde se ejecuta el servicio** o, cuando se indique, con la IP del nodo del broker.

```yaml
ingestion-service:
  build:
    context: ./ingestion-service
  image: ingestion-service:latest
  container_name: ingestion-service
  ports:
    - "5701:5701"
  command: ['datalake']
  environment:
    HZ_PORT: "5701"
    HZ_PUBLIC_ADDRESS: xxx:5701
    HZ_MEMBERS: xxx:5701
    HAZELCAST_CLUSTER_NAME: SearchEngine
    BROKER_URL: tcp://xxx:61616
    REPLICATION_FACTOR: 2
    INDEXING_BUFFER_FACTOR: 2
  volumes:
    - ./mnt/datalake:/app/datalake
  networks:
    - search_net
  profiles:
    - backend

indexing-service:
  build:
    context: ./indexing-service
  image: indexing-service:latest
  container_name: indexing-service
  ports:
    - "5702:5702"
  command: ['datalake']
  environment:
    HZ_PORT: "5702"
    HZ_PUBLIC_ADDRESS: xxx:5702
    HZ_MEMBERS: xxx:5701
    HAZELCAST_CLUSTER_NAME: SearchEngine
    BROKER_URL: tcp://xxx:61616
  volumes:
    - ./mnt/datalake:/app/datalake
  networks:
    - search_net
  profiles:
    - backend

search-service:
  build:
    context: ./search-service
  image: search-service:latest
  container_name: search-service
  ports:
    - "5703:5703"
    - "7003:7003"
  environment:
    HZ_PORT: "5703"
    SERVICE_PORT: "7003"
    HZ_PUBLIC_ADDRESS: xxx:5703
    HZ_MEMBERS: xxx:5701
    HAZELCAST_CLUSTER_NAME: SearchEngine
    SORTING_CRITERIA: "frequency"
  networks:
    - search_net
  profiles:
    - backend
```

**ParÃ¡metros relevantes**:

- `HZ_PUBLIC_ADDRESS`: DirecciÃ³n pÃºblica de este servicio, accesible por otros miembros de Hazelcast.
- `HZ_MEMBERS`: Nodo semilla (seed) usado para la formaciÃ³n inicial del clÃºster de Hazelcast.  
  Todos los nodos deben apuntar al mismo seed. En el primer nodo del sistema puede apuntar a sÃ­ mismo.
- `BROKER_URL`: DirecciÃ³n del broker de ActiveMQ.
- `REPLICATION_FACTOR`: NÃºmero de rÃ©plicas en el datalake por documento.
- `INDEXING_BUFFER_FACTOR`: Controla el batching antes de publicar eventos de indexaciÃ³n.
- `SERVICE_PORT`: Puerto HTTP expuesto por la API de bÃºsqueda.
- `SORTING_CRITERIA`: Estrategia de ordenaciÃ³n usada para los resultados de bÃºsqueda (`frequency` | `id`).

> Nota: los puertos `5701`, `5702` y `5703` se utilizan para la comunicaciÃ³n interna del clÃºster Hazelcast entre nodos.  
> El puerto `7003` es el que expone la API HTTP de bÃºsqueda hacia el exterior (y el que debe usar Nginx como backend).


### ğŸ–¥ ConfiguraciÃ³n del Balanceador de Carga (Nginx)

Antes de iniciar el balanceador de carga, el archivo `nginx.conf` debe actualizarse para incluir las direcciones IP de todos los nodos que ejecutan un servicio de bÃºsqueda. Cada entrada de backend debe apuntar a un endpoint `<NODE_IP>:7003` accesible.
```nginx
upstream search_backend {
    least_conn;

    server <NODE_IP>:7003 max_fails=10 fail_timeout=30s;
    # server <NODE_IP>:7003 max_fails=10 fail_timeout=30s;

    keepalive 64;
}
```

Agrega o elimina lÃ­neas `server` a medida que se aÃ±aden o eliminan instancias de servicio de bÃºsqueda. Nginx distribuirÃ¡ automÃ¡ticamente el trÃ¡fico y omitirÃ¡ los nodos fallidos.

### ğŸš€ Perfiles y Arranque con Docker Compose

La ejecuciÃ³n de los servicios se controla mediante **perfiles de Docker Compose** permitiendo asignar diferentes roles a distintos nodos:

- `backend`: servicios de ingestiÃ³n, indexaciÃ³n y bÃºsqueda

- `broker`: broker de mensajes ActiveMQ

- `loadbalancer`: proxy inverso Nginx

Una vez que todos los valores de configuraciÃ³n estÃ©n correctamente establecidos, se puede iniciar el cluster.

#### Nodo Principal (Broker + Backend + Load Balancer)
```bash
docker compose --profile backend --profile broker --profile loadbalancer up -d
```

#### Nodos Adicionales (Solo Servicios Backend)
```bash
docker compose --profile backend up -d
```

Cada nodo se unirÃ¡ automÃ¡ticamente al clÃºster de Hazelcast y se conectarÃ¡ al broker usando los parÃ¡metros configurados.

Para aÃ±adir nuevos nodos y escalar horizontalmente:

- Ejecuta en la nueva mÃ¡quina solo el perfil `backend`.
- Configura en ese nodo su propia IP en `HZ_PUBLIC_ADDRESS`.
- MantÃ©n el mismo valor de `HZ_MEMBERS` apuntando al nodo semilla del clÃºster.

De esta forma, el nuevo nodo se integrarÃ¡ automÃ¡ticamente en el clÃºster existente y comenzarÃ¡ a participar en la ingestiÃ³n, indexaciÃ³n y bÃºsqueda.

### ğŸ“ Notas adicionales

- Docker Compose gestiona tanto la construcciÃ³n de imÃ¡genes como la ejecuciÃ³n de contenedores; no se requiere un paso separado de `docker build`.
- Los servicios se pueden reiniciar de manera independiente sin pÃ©rdida de datos gracias a la replicaciÃ³n de Hazelcast y la coordinaciÃ³n mediante el broker.

## ğŸ“Š Benchmarking (Pruebas de Rendimiento)

### ğŸ“ˆ Resumen

Se ejecutÃ³ un conjunto de pruebas controladas (reproducibles mediante el servicio de benchmark incluido) para evaluar el **rendimiento, la escalabilidad y la tolerancia a fallos** del motor de bÃºsqueda distribuido bajo diferentes cargas de trabajo. Los experimentos se centran en:

- Tasa de ingestiÃ³n y rendimiento de indexaciÃ³n
- Latencia de consultas de bÃºsqueda bajo carga concurrente
- Escalabilidad horizontal al agregar rÃ©plicas de servicios
- Tolerancia a fallos y tiempo de recuperaciÃ³n tras fallos simulados de nodos


### ğŸ§ª ConfiguraciÃ³n del Servicio de Benchmark

```yaml
benchmark:
  environment:
    BENCHMARK_MODE: recoverytime
```

Modos de benchmark soportados:
- `ingestionrate`: documentos por segundo (docs/s)
- `indexingthroughput`: tokens por segundo (tokens/s)
- `recoverytime`: tiempo de recuperaciÃ³n del clÃºster tras fallos

### ğŸ” ReproducciÃ³n de los Benchmarks

1. Desplegar el sistema usando Docker Compose.
2. Poblar el datalake usando el servicio de ingestiÃ³n.
3. Configurar `BENCHMARK_MODE` al experimento deseado.
4. Iniciar el servicio de benchmark:
```bash
docker compose --profile benchmark up -d
```
5. Escalar los servicios backend y repetir las pruebas segÃºn sea necesario.
6. Simular fallos deteniendo contenedores durante la ejecuciÃ³n.

### â±ï¸ Benchmark de Latencia de Consultas (Apache JMeter)

La latencia de las consultas se midiÃ³ usando **Apache JMeter**. Con el sistema en ejecuciÃ³n, ejecutar:

- `load-test.jmx` (ubicado en `/benchmarks`)

El directorio `/benchmarks` tambiÃ©n contiene conjuntos de datos, registros y resultados de benchmarks anteriores.

## ğŸ¥ VÃ­deo de DemostraciÃ³n

ğŸ‘‰ [[Stage 3] Search Engine Project - GuancheData](https://youtu.be/RHMDEk85xtI)

El vÃ­deo muestra el despliegue completo del clÃºster desde cero, la ingestiÃ³n y bÃºsqueda en tiempo real, la adiciÃ³n dinÃ¡mica de nodos para escalar horizontalmente bajo carga y la recuperaciÃ³n automÃ¡tica del sistema tras la caÃ­da simulada de servicios.

## ğŸ‘¥ Autores

- **Fabio Nesta Arteaga** â€” ğŸ”— [GitHub](https://github.com/NestX10)
- **Pablo Cabeza** â€” ğŸ”— [GitHub](https://github.com/pabcablan)
- **Joel Ojeda** â€” ğŸ”— [GitHub](https://github.com/joelojeda)
- **Enrique Reina** â€” ğŸ”— [GitHub](https://github.com/ellupe)
- **Ayoze Ruano** â€” ğŸ”— [GitHub](https://github.com/ayozeruanoalc)











